<!DOCTYPE html>
<html lang="zh_CN" class="desktop-view not-mobile-device  anon">
 <head> 
  <meta charset="utf-8" /> 
  <title>面试之机器学习算法思想简单梳理 - 面试 - 算法组</title> 
  <meta name="description" content="找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。 

纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的&amp;hellip;" /> 
  <meta name="author" content="" /> 
  <meta name="generator" content="Discourse 1.6.0.beta1 - https://github.com/discourse/discourse version cc25716e475e6eed70532c8526d9e612899d61d8" /> 
  <link rel="icon" type="image/png" href="http://s1.suanfazu.com/favicon.ico" /> 
  <link rel="apple-touch-icon" type="image/png" href="/images/default-apple-touch-icon.png" /> 
  <meta name="theme-color" content="#ffffff" /> 
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=yes" /> 
  <link rel="canonical" href="http://suanfazu.com/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134" /> 
  <style>
  @font-face {
    font-family: 'FontAwesome';
    src: url('//cdn.suanfazu.com/assets/fontawesome-webfont-e2f6015310d7f63fa1537ab9822f1446.eot?http://suanfazu.com&amp;2&v=4.5.0');
    src: url('//cdn.suanfazu.com/assets/fontawesome-webfont-e2f6015310d7f63fa1537ab9822f1446.eot?http://suanfazu.com&amp;2&v=4.5.0#iefix') format('embedded-opentype'),
         url('//cdn.suanfazu.com/assets/fontawesome-webfont-6d0ddb44b6016bd7adf993e5b9d47ae6.woff2?http://suanfazu.com&amp;2&v=4.5.0') format('woff2'),
         url('//cdn.suanfazu.com/assets/fontawesome-webfont-90e687312466f7a4993c85399c116f2f.woff?http://suanfazu.com&amp;2&v=4.5.0') format('woff'),
         url('//cdn.suanfazu.com/assets/fontawesome-webfont-f436f853ea7573a6b623eea9bc9d66ec.ttf?http://suanfazu.com&amp;2&v=4.5.0') format('truetype');
    font-weight: normal;
    font-style: normal;
  }
</style> 
  <link href="//cdn.suanfazu.com/stylesheets/desktop_3cd538a579f35992edf155ad9b22c8867cb58252.css?__ws=suanfazu.com" media="all" rel="stylesheet" /> 
  <link class="custom-css" rel="stylesheet" href="//cdn.suanfazu.com/site_customizations/7e202ef2-56d7-47d5-98d8-a9c8d15e57dd.css?target=desktop&amp;v=f1b43fef7075ecbadc1dae23dd159bfb&amp;__ws=suanfazu.com" type="text/css" media="all" /> 
  <meta name="fragment" content="!" /> 
  <script>
      window.EmberENV = window.EmberENV || {};
      window.EmberENV['FORCE_JQUERY'] = true;
    </script> 
  <script src="//cdn.suanfazu.com/assets/preload_store-d16a3675434b5a0043157cfc2b850471.js"></script> 
  <script src="//cdn.suanfazu.com/assets/locales/zh_CN-d60d5bfbfe48e142f16fb1ed5d4cdc7e.js"></script> 
  <script src="//cdn.suanfazu.com/assets/ember_jquery-37c15254b70c40ceb7888cb7248c79d6.js"></script> 
  <script src="//cdn.suanfazu.com/assets/vendor-17831e059f10b3b31503434a43f32398.js"></script> 
  <script src="//cdn.suanfazu.com/assets/application-8bf39ee2538a975d4339495316dc34a1.js"></script> 
  <style>.cooked {line-height:1.6em;font-size:1.1em;} .adsense_topic_bottom{text-align:left}
.d-header .title{margin-top:10px;}
@media screen and (max-width : 650px) {
.nav-pills>li {margin-right:0px}
.nav-pills>li>a{padding:5px 6px }
}
@media screen and (max-width : 400px) {
.nav-pills>li {margin-right:0px}
.nav-pills>li>a{padding:5px 5px }
.ol.category-breadcrumb{margin-right:2px;}
}
</style> 
  <link rel="manifest" href="/manifest.json" /> 
  <link rel="alternate" type="application/rss+xml" title="'面试之机器学习算法思想简单梳理' 的 RSS 内容聚合" href="http://suanfazu.com/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134.rss" /> 
  <meta property="og:site_name" content="算法组" /> 
  <meta name="twitter:card" content="summary" /> 
  <meta property="og:url" content="http://suanfazu.com/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134" /> 
  <meta name="twitter:url" content="http://suanfazu.com/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134" /> 
  <meta property="og:title" content="面试之机器学习算法思想简单梳理" /> 
  <meta name="twitter:title" content="面试之机器学习算法思想简单梳理" /> 
  <meta property="og:description" content="找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。   纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。   下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以..." /> 
  <meta name="twitter:description" content="找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。   纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。   下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以..." /> 
  <meta property="og:image" content="http://suanfazu.com/uploads/default/42/70790b08f8d50207.png" /> 
  <meta name="twitter:image" content="http://suanfazu.com/uploads/default/42/70790b08f8d50207.png" /> 
  <meta name="twitter:label1" value="阅读时间" /> 
  <meta name="twitter:data1" value="1 mins 91" /> 
  <meta name="twitter:label2" value="赞" /> 
  <meta name="twitter:data2" value="6 78" /> 
 </head> 
 <body> 
  <noscript data-path="/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134"> 
   <header class="d-header"> 
    <div class="wrap"> 
     <div class="contents"> 
      <div class="row"> 
       <div class="title span13"> 
        <a href="/"> <h2 id="site-text-logo">算法组</h2> </a> 
       </div> 
       <div class="panel clearfix"> 
        <a href="/login" class="btn btn-primary btn-small login-button"><i class="fa fa-user"></i> 登录</a> 
       </div> 
      </div> 
     </div> 
    </div> 
   </header> 
   <div id="main-outlet" class="wrap"> 
    <!-- preload-content: --> 
    <h1> <a href="/t/mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li/134">面试之机器学习算法思想简单梳理</a> </h1> 
    <div id="breadcrumbs"> 
     <div id="breadcrumb-0" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb"> 
      <a href="/c/mian-shi" itemprop="url"> <span itemprop="title">面试</span> </a> 
     </div> 
    </div> 
    <hr /> 
    <div itemscope="" itemtype="http://schema.org/Article"> 
     <div class="creator"> 
      <span> <a href="/users/king"><b itemprop="author">king</b></a> <time datetime="2014-12-22T05:17:53Z" itemprop="datePublished"> 2014-12-22 05:17:53 UTC </time> </span> 
      <span itemprop="position">#1</span> 
     </div> 
     <div class="post" itemprop="articleBody"> 
      <p>找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。</p> 
      <p>纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。</p> 
      <p>下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以及调参经验等等。说白了，就是既要会点理论，也要会点应用，既要有点深度，也要有点广度，否则运气不好的话很容易就被刷掉，因为每个面试官爱好不同。</p> 
      <h3>朴素贝叶斯</h3> 
      <ol> 
       <li><p>如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。</p></li> 
       <li><p>计算公式如下：<br /><img src="//cdn.suanfazu.com/uploads/default/42/70790b08f8d50207.png" width="257" height="83" /> <br />其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是 <img src="//cdn.suanfazu.com/uploads/default/43/778e325ce15507a6.png" width="85" height="32" /> 的计算方法，而由朴素贝叶斯的前提假设可知，<img src="//cdn.suanfazu.com/uploads/default/44/b2ad22e455684bd6.png" width="204" height="28" /> = <img src="//cdn.suanfazu.com/uploads/default/45/3d133c5911aebce2.png" width="406" height="33" /> ，因此一般有两种，一种是在类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本的总和；第二种方法是类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本中所有特征出现次数的总和。</p></li> 
       <li><p>如果 <img src="//cdn.suanfazu.com/uploads/default/43/778e325ce15507a6.png" width="85" height="32" /> 中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑,分母加k的原因是使之满足全概率公式）。</p></li> 
      </ol> 
      <p>朴素贝叶斯优缺点：</p> 
      <ul> 
       <li>优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练</li> 
       <li>缺点：对输入数据的表达形式很敏感</li> 
      </ul> 
      <h3>决策树</h3> 
      <p>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。</p> 
      <p>信息熵的计算公式如下:</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/46/d03e716a9638cb87.png" width="308" height="56" /></p> 
      <p>其中的n代表有n个分类类别（比如假设是2类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。</p> 
      <p>现在选中一个属性xi用来进行分枝，此时分枝规则是：如果xi=vx的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’=p1*H1+p2*H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性</p> 
      <p>决策树的优缺点：</p> 
      <ul> 
       <li>优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征</li> 
       <li>缺点：容易过拟合（后续出现了随机森林，减小了过拟合现象）</li> 
      </ul> 
      <h3>Logistic回归</h3> 
      <p>Logistic是用来分类的，是一种线性分类器，需要注意的地方有：</p> 
      <p>logistic函数表达式为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/47/212ddd2a7961be08.png" width="615" height="172" /></p> 
      <p>其导数形式为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/48/4da3e4d2c40588ec.png" width="442" height="249" /></p> 
      <p>logsitc回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/49/fcb33be5d4446b37.png" width="408" height="56" /></p> 
      <p>到整个样本的后验概率：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/50/55b6dd3778879561.png" width="488" height="210" /></p> 
      <p>其中：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/51/73bab07098c38cf8.png" width="344" height="83" /></p> 
      <p>通过对数进一步化简为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/52/85191e35da45c0fc.png" width="605" height="120" /></p> 
      <p>其实它的loss function为-l(θ)，因此我们需使lossfunction最小，可采用梯度下降法得到。梯度下降法公式为:</p> 
      <p></p>
      <div class="lightbox-wrapper">
       <a data-download-href="//cdn.suanfazu.com/uploads/default/0ed682faa49c75bc1cf0d83c0f13bec43953442e" href="//cdn.suanfazu.com/uploads/default/53/09fe4e1785423904.png" class="lightbox" title="29224823-e31ad6e732d44b27a5256de58a336678.png"><img src="//cdn.suanfazu.com/uploads/default/_optimized/4d9/f49/cf5734b0b1_690x187.png" width="690" height="187" />
        <div class="meta"> 
         <span class="filename">29224823-e31ad6e732d44b27a5256de58a336678.png</span>
         <span class="informations">792x215 27.5 KB</span>
         <span class="expand"></span> 
        </div></a>
      </div>
      <p></p> 
      <p><img src="http://images.cnitblog.com/blog/381513/201310/29224841-f856f665be51466d86680c75d016cbeb.png" width="353" height="56" /></p> 
      <p>Logistic回归优缺点：</p> 
      <ul> 
       <li>优点：实现简单</li> 
       <li>优点：分类时计算量非常小，速度很快，存储资源低</li> 
       <li>缺点：容易欠拟合，一般准确度不太高</li> 
       <li>缺点：只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分</li> 
      </ul> 
      <h3>线性回归</h3> 
      <p>线性回归才是真正用于回归的，而不像logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normalequation直接求得参数的解，结果为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/54/eb3b7b2b4e35ce9b.png" width="202" height="59" /></p> 
      <p>而在LWLR（局部加权线性回归）中，参数的计算表达式为:</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/55/270097b6b4631ab8.png" width="260" height="56" /></p> 
      <p>因为此时优化的是：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/56/d78d05a1d560d3fb.png" width="474" height="95" /></p> 
      <p>由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。</p> 
      <p>线性回归优缺点：</p> 
      <ul> 
       <li>优点：实现简单，计算简单</li> 
       <li>缺点：不能拟合非线性数据</li> 
      </ul> 
      <h3>KNN算法</h3> 
      <p>KNN即最近邻算法，其主要过程为：</p> 
      <ol> 
       <li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li> 
       <li>对上面所有的距离值进行排序；</li> 
       <li>选前k个最小距离的样本；</li> 
       <li>根据这k个样本的标签进行投票，得到最后的分类类别；</li> 
      </ol> 
      <p>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。</p> 
      <p>近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。</p> 
      <p>注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下：</p> 
      <p></p>
      <div class="lightbox-wrapper">
       <a data-download-href="//cdn.suanfazu.com/uploads/default/a7b2249dc5cd0133f2437efa2059e9f336225824" href="//cdn.suanfazu.com/uploads/default/57/f648583a7428f16d.png" class="lightbox" title="29225128-2a8eb5145f3c46f6aaba72544da7d9aa.png"><img src="//cdn.suanfazu.com/uploads/default/_optimized/7b3/916/e634ca206f_690x190.png" width="690" height="190" />
        <div class="meta"> 
         <span class="filename">29225128-2a8eb5145f3c46f6aaba72544da7d9aa.png</span>
         <span class="informations">1140x315 26.8 KB</span>
         <span class="expand"></span> 
        </div></a>
      </div>
      <p></p> 
      <p>KNN算法的优缺点</p> 
      <ul> 
       <li>优点：思想简单，理论成熟，既可以用来做分类也可以用来做回归</li> 
       <li>优点：可用于非线性分类</li> 
       <li>优点：训练时间复杂度为O(n)</li> 
       <li>优点：准确度高，对数据没有假设，对outlier不敏感</li> 
       <li>缺点：计算量大</li> 
       <li>缺点：样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）</li> 
       <li>缺点：需要大量的内存</li> 
      </ul> 
      <h3>SVM</h3> 
      <p>要学会如何使用libsvm以及一些参数的调节经验，另外需要理清楚svm算法的一些思路：</p> 
      <ol> 
       <li><p>svm中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？网易深度学习岗位面试过程中有被问到。答案就是几何间隔与样本的误分次数间存在关系：<img src="//cdn.suanfazu.com/uploads/default/58/e21f3d4ae0ef0596.png" width="192" height="73" />，其中的分母就是样本到分类间隔距离，分子中的R是所有样本中的最长向量值），即：<img src="//cdn.suanfazu.com/uploads/default/59/ebb053559cd79efa.png" width="389" height="91" /><br />经过一系列推导可得为优化下面原始目标：<img src="//cdn.suanfazu.com/uploads/default/60/2c062d10e30c66fd.png" width="381" height="87" /></p></li> 
       <li><p>下面来看看拉格朗日理论：<img src="//cdn.suanfazu.com/uploads/default/61/b3fda0a6e57821e2.png" width="556" height="205" /><br />可以将1中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD条件），最后目标函数为：<img src="//cdn.suanfazu.com/uploads/default/62/be77f452468ead36.png" width="443" height="70" /><br />我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。</p></li> 
       <li><p>对2中最后的式子分别w和b求导可得：<br /><img src="//cdn.suanfazu.com/uploads/default/63/c67a6f285864d9fd.png" width="159" height="61" /><br /><img src="//cdn.suanfazu.com/uploads/default/64/c035572477d2ecb8.png" width="262" height="69" /><span></span><br />由上面第1式子可以知道，如果我们优化出了α，则直接可以求出w了，即模型的参数搞定。而上面第2个式子可以作为后续优化的一个约束条件。</p></li> 
       <li><p>对2中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数：<br /><img src="//cdn.suanfazu.com/uploads/default/65/9eadd88470f4b46f.png" width="461" height="159" /><br />而这个函数可以用常用的优化方法求得α，进而求得w和b。</p></li> 
      </ol> 
      <p>5.按照道理，svm简单理论应该到此结束。不过还是要补充一点，即在预测时有：<br /> <img src="//cdn.suanfazu.com/uploads/default/66/825b2e8fde6d1192.png" width="327" height="136" /><br /> 那个尖括号我们可以用核函数代替，这也是svm经常和核函数扯在一起的原因。</p> 
      <ol>
       <li>最后是关于松弛变量的引入，因此原始的目标优化公式为：<br /><img src="//cdn.suanfazu.com/uploads/default/67/2ce64039a41629e5.png" width="426" height="125" /><br />此时对应的对偶优化公式为：<br /><img src="//cdn.suanfazu.com/uploads/default/68/9e6fac528490476d.png" width="454" height="157" /><br />与前面的相比只是α多了个上界。</li>
      </ol> 
      <p>SVM算法优缺点</p> 
      <ul> 
       <li>优点：可用于线性/非线性分类，也可以用于回归</li> 
       <li>优点：低泛化误差</li> 
       <li>优点：容易解释</li> 
       <li>优点：计算复杂度较低</li> 
       <li>缺点：对参数和核函数的选择比较敏感</li> 
       <li>缺点：原始的SVM只比较擅长处理二分类问题</li> 
      </ul> 
      <h3>Boosting</h3> 
      <p>主要以Adaboost为例，首先来看看Adaboost的流程图，如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/69/9b036b1d241cac79.png" width="" height="" /></p> 
      <p>从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为3个），每个弱分类器是由不同权重的样本（图中为5个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？</p> 
      <p>下面通过一个例子来简单说明。</p> 
      <p>书中（machine learning inaction）假设的是5个训练样本，每个训练样本的维度为2，在训练第一个分类器时5个样本的权重各为0.2.注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。</p> 
      <p>现在假设弱分类器是带一个节点的简单决策树，该决策树会选择2个属性（假设只有2个属性）的一个，然后计算出这个属性中的最佳值用来分类。</p> 
      <p>Adaboost的简单版本训练过程如下：</p> 
      <p>1.训练第一个分类器，样本的权值D为相同的均值。通过一个弱分类器，得到这5个样本（请对应书中的例子来看，依旧是machinelearning inaction）的分类预测标签。与给出的样本真实标签对比，就可能出现误差(即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为0.最后累加5个样本的错误率之和，记为ε。</p> 
      <ol>
       <li>通过ε来计算该弱分类器的权重α，公式如下：</li>
      </ol> 
      <p><img src="//cdn.suanfazu.com/uploads/default/70/46d21b97906fdf23.png" width="188" height="81" /></p> 
      <p>3.通过α来计算训练下一个弱分类器样本的权重D，如果对应样本分类正确，则减小该样本的权重，公式为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/71/df8065d2cac1626b.png" width="" height="" /></p> 
      <p>如果样本分类错误，则增加该样本的权重，公式为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/72/f1693cfba11058c1.png" width="" height="" /></p> 
      <ol>
       <li>循环步骤1,2,3来继续训练多个分类器，只是其D值不同而已。</li>
      </ol> 
      <p>测试过程如下：</p> 
      <p>输入一个样本到训练好的每个弱分类中，则每个弱分类都对应一个输出标签，然后该标签乘以对应的α，最后求和得到值的符号即为预测标签值。</p> 
      <p>Boosting算法的优缺点：</p> 
      <ul> 
       <li>优点：低泛化误差</li> 
       <li>优点：容易实现，分类准确率较高，没有太多参数可以调</li> 
       <li>缺点：对outlier比较敏感</li> 
      </ul> 
      <h3>聚类：</h3> 
      <p>根据聚类思想划分：</p> 
      <ol> 
       <li>基于划分的聚类:<br />K-means,k-medoids(每一个类别中找一个样本点来代表),CLARANS.<br />k-means是使下面的表达式值最小：<br /><img src="//cdn.suanfazu.com/uploads/default/73/d91ff1331bdb0dec.png" width="" height="" /><br />k-means算法的优缺点：
        <ul> 
         <li><p>优点：k-means算法是解决聚类问题的一种经典算法，算法简单、快速</p></li> 
         <li>优点：对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛</li> 
         <li>优点：算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好</li> 
         <li>缺点：k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合</li> 
         <li>缺点：要求用户必须事先给出要生成的簇的数目k</li> 
         <li>缺点：对初值敏感，对于不同的初始值，可能会导致不同的聚类结果</li> 
         <li>缺点：不适合于发现非凸面形状的簇，或者大小差别很大的簇</li> 
         <li>缺点：对于&quot;噪声&quot;和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响</li> 
        </ul> </li> 
       <li><p>基于层次的聚类：<br />自底向上的凝聚方法，比如AGNES。<br />自上向下的分裂方法，比如DIANA。</p></li> 
       <li><p>基于密度的聚类：<br />DBSACN,OPTICS,BIRCH(CF-Tree),CURE.</p></li> 
       <li><p>基于网格的方法：<br />STING, WaveCluster.</p></li> 
       <li><p>基于模型的聚类：<br />EM,SOM,COBWEB.</p></li> 
      </ol> 
      <h3>推荐系统：</h3> 
      <p>推荐系统的实现主要分为两个方面：基于内容的实现和协同滤波的实现。</p> 
      <p><strong>基于内容的实现</strong></p> 
      <p>不同人对不同电影的评分这个例子，可以看做是一个普通的回归问题，因此每部电影都需要提前提取出一个特征向量(即x值)，然后针对每个用户建模，即每个用户打的分值作为y值，利用这些已有的分值y和电影特征值x就可以训练回归模型了(最常见的就是线性回归)。这样就可以预测那些用户没有评分的电影的分数。（值得注意的是需对每个用户都建立他自己的回归模型）</p> 
      <p>从另一个角度来看，也可以是先给定每个用户对某种电影的喜好程度（即权值），然后学出每部电影的特征，最后采用回归来预测那些没有被评分的电影。</p> 
      <p>当然还可以是同时优化得到每个用户对不同类型电影的热爱程度以及每部电影的特征。具体可以参考Ng在coursera上的ml教程：https://www.coursera.org/course/ml</p> 
      <p><strong>基于协同滤波的实现</strong></p> 
      <p>协同滤波（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。</p> 
      <p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/74/2ad28a75168d4b7b.png" width="" height="" /></p> 
      <p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/75/ff909fdccebadfe5.png" width="" height="" /></p> 
      <p>其中更深的颜色代表去掉小特征值重构时的三个矩阵。</p> 
      <p>果m代表商品的个数，n代表用户的个数，则U矩阵的每一行代表商品的属性，现在通过降维U矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为k维）。这样当新来一个用户的商品推荐向量X，则可以根据公式X'<em>U1</em>inv(S1)得到一个k维的向量，然后在V’中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。具体例子可以参考网页：SVD在推荐系统中的应用。</p> 
      <p>另外关于SVD分解后每个矩阵的实际含义可以参考google吴军的《数学之美》一书（不过个人感觉吴军解释UV两个矩阵时好像弄反了，不知道大家怎样认为）。或者参考machinelearning in action其中的svd章节。</p> 
      <h3>pLSA:</h3> 
      <p>pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。pLSA的模型图如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/76/7df43e6a15fcf8ce.png" width="220" height="74" /></p> 
      <p>公式中的意义如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/77/818606e36fcb74b1.png" width="551" height="204" /></p> 
      <h3>LDA</h3> 
      <p>主题模型，概率图如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/78/17f37911653e1fc4.jpg" width="438" height="355" /></p> 
      <p>和pLSA不同的是LDA中假设了很多先验分布，且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。</p> 
      <h3>GDBT</h3> 
      <p>GBDT(Gradient Boosting Decision Tree)又叫MART（Multiple Additive Regression Tree)，好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。</p> 
      <p>GBDT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。</p> 
      <p>关于GDBT的介绍可以参考 <br /></p>
      <aside class="quote" data-post="1" data-topic="133"> 
       <div class="title"> 
        <div class="quote-controls"></div> 
        <img width="20" height="20" src="/letter_avatar/king/40/2.png" class="avatar" /> 
        <a href="http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/133">Gbdt 迭代决策树入门教程</a> 
        <a href="/category/ji-qi-xue-xi" class="badge badge-category" style="background-color: #3AB54A; color: #FFFFFF">机器学习</a> 
       </div> 
       <blockquote>
        GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种用于回归的机器学习算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。当把目标函数做变换后，该算法亦可用于分类或排序。 本文主要从高层明确几个GBDT概念，主要讲GBDT的两个版本以及GBDT是什么不是什么。详细介绍见文中的链接。 GBDT的两个不同版本 目前GBDT有两个不同的描述版本，两者各有支持者，读文献时要注意区分。残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差，之前我写的GBDT入门教程主要在描述这一版本，ELF开源软件实现中用的也是这一版本。Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值，之前leftnoteasy的博客中介绍的为此版本，umass的源码实现中用的则是这一版本（准确的说是LambdaMART中的MART为这一版本，MART实现则是前一版本）。 对GBDT无基础的朋友可… 
        <div class="topic-info"> 
        </div> 
       </blockquote> 
      </aside> 
      <p></p> 
      <h3>Regularization:</h3> 
      <p>作用是（网易电话面试时有问到）：</p> 
      <ol> 
       <li>数值上更容易求解</li> 
       <li>特征数目太大时更稳定<br />3.控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑</li> 
       <li>减小参数空间；参数空间越小，复杂度越低<br />5.系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）</li> 
       <li>可以看出是权值的高斯先验</li> 
      </ol> 
      <h3>异常检测</h3> 
      <p>可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。如果样本有n维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说x=log(x+c),x=x^(1/c)等。异常检测的算法流程如下：</p> 
      <p></p>
      <div class="lightbox-wrapper">
       <a data-download-href="//cdn.suanfazu.com/uploads/default/e1f6bb7c8d1cff6d95504683556681719058f476" href="//cdn.suanfazu.com/uploads/default/79/7e2de99b523dbff9.png" class="lightbox" title="29230143-aa4642e8d66a468b9138e9014b2ec704.png"><img src="//cdn.suanfazu.com/uploads/default/_optimized/71b/c66/2846715f92_690x415.png" width="690" height="415" />
        <div class="meta"> 
         <span class="filename">29230143-aa4642e8d66a468b9138e9014b2ec704.png</span>
         <span class="informations">813x490 66.5 KB</span>
         <span class="expand"></span> 
        </div></a>
      </div>
      <p></p> 
      <p>其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的p(x)的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。</p> 
      <p>另外，上面是将特征的每一维看成是相互独立的高斯分布，其实这样的近似并不是最好的，但是它的计算量较小，因此也常被使用。更好的方法应该是将特征拟合成多维高斯分布，这时有特征之间的相关性，但随之计算量会变复杂，且样本的协方差矩阵还可能出现不可逆的情况（主要在样本数比特征数小，或者样本特征维数之间有线性关系时）。</p> 
      <p>上面的内容可以参考Ng的 <a href="https://www.coursera.org/course/ml" rel="nofollow">https://www.coursera.org/course/ml</a></p> 
      <h3>EM算法</h3> 
      <p>有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：</p> 
      <ol> 
       <li>E步：选取一组参数，求出在该参数下隐含变量的条件概率值</li> 
       <li>M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值</li> 
      </ol> 
      <p>重复上面2步直至收敛</p> 
      <p>公式如下所示：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/80/9cf57f443d28dd5a.png" width="" height="" /></p> 
      <p>M步公式中下界函数的推导过程：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/81/e1d8f6d9384a55b1.png" width="" height="" /></p> 
      <p>EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。</p> 
      <p>GMM的E步公式如下（计算每个样本对应每个高斯的概率）：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/82/cebc0e1f7f830dbc.png" width="" height="" /></p> 
      <p>更具体的计算公式为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/83/359c905e05fde36f.png" width="" height="" /></p> 
      <p>M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/84/bb2dd6bdcbb1bc0b.png" width="" height="" /></p> 
      <p>关于EM算法可以参考Ng的cs229课程资料 或者网易公开课：斯坦福大学公开课：机器学习课程。</p> 
      <h3>Apriori:</h3> 
      <p>Apriori是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：</p> 
      <p>1.如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合<br />2.如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合</p> 
      <p>Aprioir需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为L，然后对L中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为C，接着又扫描去掉那些非频繁的项目，重复…</p> 
      <p>看下面这个例子：</p> 
      <p>元素项目表格：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/85/940f014b9738148a.png" width="" height="" /></p> 
      <p>如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/86/01a37040a7ca3a44.png" width="" height="" /></p> 
      <p>在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/87/86a71a3e8aef68d1.png" width="360" height="342" /></p> 
      <p>上面的内容主要参考的是machine learningin action这本书。</p> 
      <h3>FPGrowth:</h3> 
      <p>FPGrowth是一种比Apriori更高效的频繁项挖掘方法，它只需要扫描项目表2次。其中第1次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第2遍扫描是建立一颗FP-Tree(frequent-pattentree)。</p> 
      <p>接下来的工作就是在FP-Tree上进行挖掘。</p> 
      <p>比如说有下表：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/88/4c045a7c07f58cde.png" width="537" height="239" /></p> 
      <p>它所对应的FP_Tree如下：</p> 
      <p><img src="//cdn.suanfazu.com/uploads/default/89/2a125f846556f88a.png" width="" height="" /></p> 
      <p>然后从频率最小的单项P开始，找出P的条件模式基，用构造FP_Tree同样的方法来构造P的条件模式基的FP_Tree，在这棵树上找出包含P的频繁项集。</p> 
      <p>依次从m,b,a,c,f的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如m节点，具体的过程可以参考博客：FrequentPattern挖掘之二(FPGrowth算法)，里面讲得很详细。</p> 
     </div> 
     <meta itemprop="interactionCount" content="UserLikes:6" /> 
     <meta itemprop="interactionCount" content="UserComments:0" /> 
     <hr /> 
    </div> 
    <!-- :preload-content --> 
    <footer> 
     <nav itemscope="" itemtype="http://schema.org/SiteNavigationElement"> 
      <a href="/">主页</a> 
      <a href="/categories">分类</a> 
      <a href="/guidelines">FAQ/指引</a> 
      <a href="/tos">服务条款</a> 
      <a href="/privacy">隐私政策</a> 
     </nav> 
    </footer> 
   </div> 
   <footer id="noscript-footer"> 
    <p>采用 <a href="http://www.discourse.org">Discourse</a>，启用 JavaScript 以获得最佳效果</p> 
   </footer> 
  </noscript> 
  <section id="main"> 
  </section> 
  <div id="offscreen-content"> 
  </div> 
  <form id="hidden-login-form" method="post" action="/login" style="display: none;"> 
   <input name="username" type="text" id="signin_username" /> 
   <input name="password" type="password" id="signin_password" /> 
   <input name="redirect" type="hidden" /> 
   <input type="submit" id="signin-button" value="登录" /> 
  </form> 
  <script>
        PreloadStore.store("site",{"default_archetype":"regular","notification_types":{"mentioned":1,"replied":2,"quoted":3,"edited":4,"liked":5,"private_message":6,"invited_to_private_message":7,"invitee_accepted":8,"posted":9,"moved_post":10,"linked":11,"granted_badge":12,"invited_to_topic":13,"custom":14,"group_mentioned":15,"group_message_summary":16},"post_types":{"regular":1,"moderator_action":2,"small_action":3,"whisper":4},"groups":[{"id":1,"name":"admins"},{"id":0,"name":"everyone"},{"id":2,"name":"moderators"},{"id":3,"name":"staff"},{"id":10,"name":"trust_level_0"},{"id":11,"name":"trust_level_1"},{"id":12,"name":"trust_level_2"},{"id":13,"name":"trust_level_3"},{"id":14,"name":"trust_level_4"},{"id":41,"name":"WeUseCaffe"}],"filters":["latest","unread","new","read","posted","bookmarks"],"periods":["all","yearly","quarterly","monthly","weekly","daily"],"top_menu_items":["latest","unread","new","read","posted","bookmarks","category","categories","top"],"anonymous_top_menu_items":["latest","top","categories","category","categories","top"],"uncategorized_category_id":1,"is_readonly":false,"disabled_plugins":[],"user_field_max_length":2048,"suppressed_from_homepage_category_ids":[18,24,26],"post_action_types":[{"name_key":"bookmark","name":"书签","description":"给本帖加书签","long_form":"已给本帖加上书签","is_flag":false,"icon":null,"id":1,"is_custom_flag":false},{"name_key":"like","name":"赞","description":"赞本帖","long_form":"赞本帖内容","is_flag":false,"icon":"heart","id":2,"is_custom_flag":false},{"name_key":"off_topic","name":"题外话","description":"此帖与该主题标题和第一帖而言所讨论的主题无关，可能需要被移动。","long_form":"标记为题外话","is_flag":true,"icon":null,"id":3,"is_custom_flag":false},{"name_key":"inappropriate","name":"不当内容","description":"此帖内容包含对他人的攻击、侮辱、仇视语言或违反了<a href=\"/guidelines\">我们的社群准则<\/a>。","long_form":"标记为不当内容","is_flag":true,"icon":null,"id":4,"is_custom_flag":false},{"name_key":"vote","name":"投票","description":"给本帖投票","long_form":"已给本帖投票","is_flag":false,"icon":null,"id":5,"is_custom_flag":false},{"name_key":"spam","name":"垃圾","description":"此帖为广告。它不包含任何对当前讨论有帮助的内容，只有促销信息。","long_form":"标记为垃圾","is_flag":true,"icon":null,"id":8,"is_custom_flag":false},{"name_key":"notify_user","name":"给@{{username}}发送一条消息","description":"我想与此人私下交流他们的帖子。","long_form":"以发送消息给用户","is_flag":true,"icon":null,"id":6,"is_custom_flag":true},{"name_key":"notify_moderators","name":"其他事项","description":"这个帖子需要版主的注意，原因没有列在上方。","long_form":"标记为需版主注意","is_flag":true,"icon":null,"id":7,"is_custom_flag":true}],"topic_flag_types":[{"name_key":"inappropriate","name":"不当内容","description":"此主题内容包含对他人的攻击、侮辱、仇视语言或违反了<a href=\"/guidelines\">我们的社群准则<\/a>。","long_form":"标记为不当内容","is_flag":true,"icon":null,"id":4,"is_custom_flag":false},{"name_key":"spam","name":"垃圾","description":"这个主题是广告。它对本站点没有联系和帮助，仅仅是推销信息。","long_form":"标记为垃圾","is_flag":true,"icon":null,"id":8,"is_custom_flag":false},{"name_key":"notify_moderators","name":"其他内容","description":"此帖需要版主依据<a href=\"/guidelines\">社群准则<\/a>、<a href=\"/tos\">服务条款（TOS）<\/a>或其它未列出的原因来给予关注。","long_form":"标记为需版主注意","is_flag":true,"icon":null,"id":7,"is_custom_flag":true}],"can_create_tag":null,"can_tag_topics":null,"tags_filter_regexp":"[<\\\\/\\>\\#\\?\\&\\s]","categories":[{"id":1,"name":"其他","color":"AB9364","text_color":"FFFFFF","slug":"qi-ta","topic_count":1,"post_count":1,"position":0,"description":"不需要分类或者不适合放在现在的任何分类中的主题。","description_text":"","topic_url":"/t//","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":6,"name":"算法","color":"12A89D","text_color":"FFFFFF","slug":"suan-fa","topic_count":24,"post_count":45,"position":1,"description":"关于算法的讨论区。","description_text":"关于算法的讨论区。","topic_url":"/t/guan-yu-fen-lei-suan-fa/12","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":5,"name":"机器学习","color":"3AB54A","text_color":"FFFFFF","slug":"ji-qi-xue-xi","topic_count":113,"post_count":182,"position":2,"description":"机器学习讨论区。","description_text":"机器学习讨论区。","topic_url":"/t/guan-yu-fen-lei-ji-qi-xue-xi/11","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":true},{"id":8,"name":"工程开发","color":"F1592A","text_color":"FFFFFF","slug":"gong-cheng-kai-fa","topic_count":40,"post_count":73,"position":3,"description":"开发、代码、编程、工程实现相关讨论","description_text":"开发、代码、编程、工程实现相关讨论","topic_url":"/t/guan-yu-fen-lei-gong-cheng-he-dai-ma/16","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":12,"name":"面试","color":"25AAE2","text_color":"FFFFFF","slug":"mian-shi","topic_count":14,"post_count":34,"position":4,"description":"面试题，面试经验","description_text":"面试题，面试经验","topic_url":"/t/guan-yu-fen-lei-mian-shi/96","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":13,"name":"工作招聘","color":"9EB83B","text_color":"FFFFFF","slug":"gong-zuo-zhao-pin","topic_count":9,"post_count":10,"position":5,"description":"招聘职位信息，工作相关讨论","description_text":"招聘职位信息，工作相关讨论","topic_url":"/t/guan-yu-fen-lei-gong-zuo-zhao-pin/234","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":9,"name":"创业","color":"F7941D","text_color":"FFFFFF","slug":"chuang-ye","topic_count":3,"post_count":3,"position":6,"description":null,"description_text":null,"topic_url":"/t/guan-yu-fen-lei-chuang-ye/21","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":17,"name":"领域应用","color":"B3B5B4","text_color":"FFFFFF","slug":"application","topic_count":1,"post_count":1,"position":7,"description":null,"description_text":null,"topic_url":"/t/guan-yu-fen-lei-ling-yu-ying-yong/462","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":true},{"id":10,"name":"程序化交易","color":"ED207B","text_color":"FFFFFF","slug":"cheng-xu-hua-jiao-yi","topic_count":4,"post_count":5,"position":8,"description":"程序化交易，自动化交易，量化投资，交易策略和算法讨论。","description_text":"程序化交易，自动化交易，量化投资，交易策略和算法讨论。","topic_url":"/t/guan-yu-fen-lei-cheng-xu-hua-jiao-yi/27","logo_url":"","background_url":"","read_restricted":false,"permission":null,"parent_category_id":17,"notification_level":1,"topic_template":null,"has_children":false},{"id":7,"name":"Deep Learning","color":"0E76BD","text_color":"FFFFFF","slug":"deep-learning","topic_count":38,"post_count":76,"position":11,"description":"深度学习（Deep Learning）。","description_text":"深度学习（Deep Learning）。","topic_url":"/t/guan-yu-fen-lei-deep-learning/13","logo_url":null,"background_url":null,"read_restricted":false,"permission":null,"parent_category_id":5,"notification_level":1,"topic_template":null,"has_children":false},{"id":11,"name":"幽默","color":"3366aa","text_color":"FFFFFF","slug":"joke","topic_count":10,"post_count":20,"position":12,"description":"幽默笑话。程序员的笑点。","description_text":"幽默笑话。程序员的笑点。","topic_url":"/t/guan-yu-fen-lei-you-mo-xiao-hua/30","logo_url":"","background_url":"","read_restricted":false,"permission":null,"parent_category_id":15,"notification_level":1,"topic_template":null,"has_children":false},{"id":16,"name":"有图","color":"DD2C4C","text_color":"FFFFFF","slug":"mm","topic_count":1,"post_count":4,"position":13,"description":"有图有真相，“多看美女能长寿”。","description_text":"有图有真相，“多看美女能长寿”。","topic_url":"/t/guan-yu-fen-lei-mei-ren-mei-jing/456","logo_url":"","background_url":"","read_restricted":false,"permission":null,"parent_category_id":15,"notification_level":1,"topic_template":null,"has_children":false},{"id":3,"name":"社区建设","color":"808281","text_color":"FFFFFF","slug":"she-qu-jian-she","topic_count":6,"post_count":10,"position":14,"description":"关于本算法组社区建设的反馈、建议和讨论。","description_text":"关于本算法组社区建设的反馈、建议和讨论。","topic_url":"/t/about-the-meta-category/2","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":14,"name":"博客","color":"008B45","text_color":"FFFFFF","slug":"blog","topic_count":2,"post_count":2,"position":15,"description":"你的个人博客文章。如果你想发布/记录个人的博客文章，可以放到此分类。此分类的类容，不会显示到首页上。","description_text":"你的个人博客文章。如果你想发布/记录个人的博客文章，可以放到此分类。此分类的类容，不会显示到首页上。","topic_url":"/t/guan-yu-fen-lei-bo-ke/451","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":15,"name":"有趣","color":"1E90FF","text_color":"FFFFFF","slug":"fun","topic_count":0,"post_count":1,"position":16,"description":"我们总有些无聊着的时候：程序在编译，文件在上传，车还没来，…，不要无聊着，来这里看看段子图片…","description_text":"我们总有些无聊着的时候：程序在编译，文件在上传，车还没来，…，不要无聊着，来这里看看段子图片…","topic_url":"/t/guan-yu-fen-lei-bu-xu-wu-liao/454","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":true},{"id":18,"name":"精选","color":"231F20","text_color":"FFFFFF","slug":"z","topic_count":12139,"post_count":35631,"position":17,"description":"精选文章，来自本站或者外站。","description_text":"精选文章，来自本站或者外站。","topic_url":"/t/topic/734","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":"","has_children":false},{"id":22,"name":"Caffe","color":"C30000","text_color":"FFFFFF","slug":"caffe","topic_count":15,"post_count":55,"position":18,"description":"Caffe 机器学习中文社区","description_text":"Caffe 机器学习中文社区","topic_url":"/t/caffe/12323","logo_url":null,"background_url":null,"read_restricted":false,"permission":null,"parent_category_id":5,"notification_level":1,"topic_template":null,"has_children":false},{"id":23,"name":"MXNet","color":"2980B9","text_color":"FFFFFF","slug":"mxnet","topic_count":7,"post_count":9,"position":19,"description":null,"description_text":null,"topic_url":"/t/mxnet/12324","logo_url":null,"background_url":null,"read_restricted":false,"permission":null,"parent_category_id":5,"notification_level":1,"topic_template":null,"has_children":false},{"id":24,"name":"头条","color":"BF1E2E","text_color":"FFFFFF","slug":"toutiao","topic_count":14,"post_count":15,"position":20,"description":"你必须关注的文章：技术、业界、创业等等","description_text":"你必须关注的文章：技术、业界、创业等等","topic_url":"/t/topic/12808","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":"","has_children":false},{"id":25,"name":"TensorFlow","color":"92278F","text_color":"FFFFFF","slug":"tensorflow","topic_count":7,"post_count":8,"position":21,"description":"关于TensorFlow的讨论和分享等一切。","description_text":"关于TensorFlow的讨论和分享等一切。","topic_url":"/t/tensorflow/13214","logo_url":null,"background_url":null,"read_restricted":false,"permission":null,"parent_category_id":5,"notification_level":1,"topic_template":null,"has_children":false},{"id":26,"name":"教程","color":"652D90","text_color":"FFFFFF","slug":"tutorial","topic_count":12,"post_count":12,"position":22,"description":"快速学习教程，在线入门到高级教程","description_text":"快速学习教程，在线入门到高级教程","topic_url":"/t/topic/13398","logo_url":"","background_url":"","read_restricted":false,"permission":null,"notification_level":1,"topic_template":"","has_children":true},{"id":27,"name":"算法竞赛","color":"8C6238","text_color":"FFFFFF","slug":"contests","topic_count":4,"post_count":14,"position":23,"description":null,"description_text":null,"topic_url":"/t/topic/13456","logo_url":null,"background_url":null,"read_restricted":false,"permission":null,"notification_level":1,"topic_template":null,"has_children":false},{"id":28,"name":"教程系列","color":"99cccc","text_color":"FFFFFF","slug":"tutorial-list","topic_count":4,"post_count":4,"position":24,"description":"在这个分类里存放教程目录，便于查询","description_text":"在这个分类里存放教程目录，便于查询","topic_url":"/t/topic/13469","logo_url":"","background_url":"","read_restricted":false,"permission":null,"parent_category_id":26,"notification_level":1,"topic_template":"","has_children":false}],"trust_levels":[{"id":0,"name":"新用户"},{"id":1,"name":"初级用户"},{"id":2,"name":"成员"},{"id":3,"name":"活跃用户"},{"id":4,"name":"资深"}],"archetypes":[{"id":"regular","name":"常规主题","options":[]},{"id":"banner","name":"横幅主题","options":[]}],"user_fields":[]});
        PreloadStore.store("siteSettings",{"title":"算法组","contact_email":"suanfazu@suanfazu.com","contact_url":"","logo_url":"","logo_small_url":"","mobile_logo_url":"","favicon_url":"http://s1.suanfazu.com/favicon.ico","allow_user_locale":false,"suggested_topics":10,"track_external_right_clicks":false,"ga_universal_tracking_code":"","ga_universal_domain_name":"auto","ga_tracking_code":"","ga_domain_name":"","top_menu":"latest|new|unread|top|categories|category/头条","post_menu":"like-count|like|share|flag|edit|bookmark|delete|admin|reply","post_menu_hidden_items":"bookmark|edit|delete|admin","share_links":"weibo|wechat|renren|twitter|facebook|google+|email","category_colors":"BF1E2E|F1592A|F7941D|9EB83B|3AB54A|12A89D|25AAE2|0E76BD|652D90|92278F|ED207B|8C6238|231F20|808281|B3B5B4|283890","category_style":"bullet","enable_mobile_theme":true,"relative_date_duration":30,"category_featured_topics":3,"fixed_category_positions":true,"fixed_category_positions_on_create":false,"show_subcategory_list":false,"enable_badges":true,"enable_whispers":false,"invite_only":false,"login_required":false,"must_approve_users":false,"enable_local_logins":true,"allow_new_registrations":true,"enable_signup_cta":true,"enable_google_oauth2_logins":true,"enable_yahoo_logins":false,"enable_twitter_logins":false,"enable_instagram_logins":false,"enable_facebook_logins":false,"enable_github_logins":true,"enable_sso":false,"sso_overrides_avatar":false,"min_username_length":3,"max_username_length":20,"min_password_length":6,"min_admin_password_length":15,"logout_redirect":"","full_name_required":false,"enable_names":true,"invites_per_page":40,"delete_user_max_post_age":60,"delete_all_posts_max":15,"show_email_on_profile":false,"enable_user_directory":true,"allow_anonymous_posting":false,"anonymous_posting_min_trust_level":1,"hide_user_profiles_from_public":false,"min_post_length":7,"min_first_post_length":10,"min_private_message_post_length":5,"max_post_length":232000,"min_topic_title_length":4,"max_topic_title_length":255,"min_private_message_title_length":2,"allow_uncategorized_topics":false,"min_title_similar_length":10,"min_body_similar_length":15,"enable_private_messages":true,"edit_history_visible_to_public":false,"delete_removed_posts_after":24,"traditional_markdown_linebreaks":false,"allow_html_tables":false,"suppress_reply_directly_below":true,"suppress_reply_directly_above":true,"max_reply_history":1,"newuser_max_images":1,"newuser_max_attachments":0,"display_name_on_posts":false,"show_time_gap_days":7,"short_progress_text_threshold":10000,"default_code_lang":"auto","autohighlight_all_code":false,"highlighted_languages":"apache|bash|cs|cpp|css|coffeescript|diff|xml|http|ini|json|java|javascript|makefile|markdown|nginx|objectivec|ruby|perl|php|python|sql|handlebars","censored_words":"","enable_emoji":true,"emoji_set":"emoji_one","email_time_window_mins":10,"disable_digest_emails":false,"email_in":false,"disable_emails":false,"max_image_size_kb":10240,"max_attachment_size_kb":10240,"authorized_extensions":"jpg|jpeg|png|gif|pdf|zip|pptx|txt","max_image_width":690,"max_image_height":5000,"prevent_anons_from_downloading_files":true,"enable_s3_uploads":false,"allow_profile_backgrounds":true,"allow_uploaded_avatars":true,"allow_animated_avatars":false,"default_avatars":"","external_system_avatars_enabled":true,"external_system_avatars_url":"/letter_avatar_proxy/v2/letter/{first_letter}/{color}/{size}.png","tl1_requires_read_posts":2,"tl3_links_no_follow":false,"use_admin_ip_whitelist":false,"alert_admins_if_errors_per_minute":0,"alert_admins_if_errors_per_hour":0,"enable_long_polling":true,"long_polling_base_url":"/","background_polling_interval":60000,"polling_interval":3000,"anon_polling_interval":15000,"flush_timings_secs":20,"verbose_localization":false,"max_new_topics":500,"tos_url":"","privacy_policy_url":"","faq_url":"","maximum_backups":5,"version_checks":true,"suppress_uncategorized_badge":true,"min_search_term_length":1,"topic_views_heat_low":1000,"topic_views_heat_medium":2000,"topic_views_heat_high":5000,"topic_post_like_heat_low":0.5,"topic_post_like_heat_medium":1.0,"topic_post_like_heat_high":2.0,"history_hours_low":12,"history_hours_medium":24,"history_hours_high":48,"cold_age_days_low":14,"cold_age_days_medium":90,"cold_age_days_high":180,"global_notice":"","show_create_topics_notice":true,"automatically_unpin_topics":true,"read_time_word_count":500,"disable_mailing_list_mode":false,"default_topics_automatic_unpin":true,"poll_enabled":true,"poll_maximum_options":100,"tagging_enabled":true,"max_tags_per_topic":5,"max_tag_length":20,"min_trust_level_to_tag_topics":0,"max_tag_search_results":5,"show_filter_by_tag":false,"tags_sort_alphabetically":false,"staff_tags":"","suppress_overlapping_tags_in_list":false,"details_enabled":true,"enable_mathjax_plugin":true,"mathjax_url":"//cdn.bootcss.com/mathjax/2.6.1/MathJax.js","mathjax_config":"TeX-AMS-MML_HTMLorMML","available_locales":"ar|bs_BA|cs|da|de|en|es|fa_IR|fi|fr|gl|he|id|it|ja|ko|nb_NO|nl|pl_PL|pt|pt_BR|ro|ru|sk|sq|sv|te|tr_TR|uk|vi|zh_CN|zh_TW","tag_style":"simple"});
        PreloadStore.store("customHTML",{"top":"\n","footer":"\n"});
        PreloadStore.store("banner",{});
        PreloadStore.store("customEmoji",[]);
        PreloadStore.store("translationOverrides",{});
        PreloadStore.store("topic_134",{"post_stream":{"posts":[{"id":138,"name":"king","username":"king","avatar_template":"/letter_avatar_proxy/v2/letter/k/a9adbd/{size}.png","created_at":"2014-12-22T05:17:53.583Z","cooked":"<p>找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。<\/p>\n\n<p>纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。<\/p>\n\n<p>下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以及调参经验等等。说白了，就是既要会点理论，也要会点应用，既要有点深度，也要有点广度，否则运气不好的话很容易就被刷掉，因为每个面试官爱好不同。<\/p>\n\n<h3>朴素贝叶斯<\/h3>\n\n<ol>\n<li><p>如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。<\/p><\/li>\n<li><p>计算公式如下：<br><img src=\"//cdn.suanfazu.com/uploads/default/42/70790b08f8d50207.png\" width=\"257\" height=\"83\"> <br>其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是 <img src=\"//cdn.suanfazu.com/uploads/default/43/778e325ce15507a6.png\" width=\"85\" height=\"32\"> 的计算方法，而由朴素贝叶斯的前提假设可知，<img src=\"//cdn.suanfazu.com/uploads/default/44/b2ad22e455684bd6.png\" width=\"204\" height=\"28\"> = <img src=\"//cdn.suanfazu.com/uploads/default/45/3d133c5911aebce2.png\" width=\"406\" height=\"33\"> ，因此一般有两种，一种是在类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本的总和；第二种方法是类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本中所有特征出现次数的总和。<\/p><\/li>\n<li><p>如果 <img src=\"//cdn.suanfazu.com/uploads/default/43/778e325ce15507a6.png\" width=\"85\" height=\"32\"> 中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑,分母加k的原因是使之满足全概率公式）。<\/p><\/li>\n<\/ol>\n\n<p>朴素贝叶斯优缺点：<\/p>\n\n<ul>\n<li>优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练<\/li>\n<li>缺点：对输入数据的表达形式很敏感<\/li>\n<\/ul>\n\n<h3>决策树<\/h3>\n\n<p>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。<\/p>\n\n<p>信息熵的计算公式如下:<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/46/d03e716a9638cb87.png\" width=\"308\" height=\"56\"><\/p>\n\n<p>其中的n代表有n个分类类别（比如假设是2类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。<\/p>\n\n<p>现在选中一个属性xi用来进行分枝，此时分枝规则是：如果xi=vx的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’=p1*H1+p2*H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性<\/p>\n\n<p>决策树的优缺点：<\/p>\n\n<ul>\n<li>优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征<\/li>\n<li>缺点：容易过拟合（后续出现了随机森林，减小了过拟合现象）<\/li>\n<\/ul>\n\n<h3>Logistic回归<\/h3>\n\n<p>Logistic是用来分类的，是一种线性分类器，需要注意的地方有：<\/p>\n\n<p>logistic函数表达式为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/47/212ddd2a7961be08.png\" width=\"615\" height=\"172\"><\/p>\n\n<p>其导数形式为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/48/4da3e4d2c40588ec.png\" width=\"442\" height=\"249\"><\/p>\n\n<p>logsitc回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/49/fcb33be5d4446b37.png\" width=\"408\" height=\"56\"><\/p>\n\n<p>到整个样本的后验概率：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/50/55b6dd3778879561.png\" width=\"488\" height=\"210\"><\/p>\n\n<p>其中：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/51/73bab07098c38cf8.png\" width=\"344\" height=\"83\"><\/p>\n\n<p>通过对数进一步化简为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/52/85191e35da45c0fc.png\" width=\"605\" height=\"120\"><\/p>\n\n<p>其实它的loss function为-l(θ)，因此我们需使lossfunction最小，可采用梯度下降法得到。梯度下降法公式为:<\/p>\n\n<p><div class=\"lightbox-wrapper\"><a data-download-href=\"//cdn.suanfazu.com/uploads/default/0ed682faa49c75bc1cf0d83c0f13bec43953442e\" href=\"//cdn.suanfazu.com/uploads/default/53/09fe4e1785423904.png\" class=\"lightbox\" title=\"29224823-e31ad6e732d44b27a5256de58a336678.png\"><img src=\"//cdn.suanfazu.com/uploads/default/_optimized/4d9/f49/cf5734b0b1_690x187.png\" width=\"690\" height=\"187\"><div class=\"meta\">\n<span class=\"filename\">29224823-e31ad6e732d44b27a5256de58a336678.png<\/span><span class=\"informations\">792x215 27.5 KB<\/span><span class=\"expand\"><\/span>\n<\/div><\/a><\/div><\/p>\n\n<p><img src=\"http://images.cnitblog.com/blog/381513/201310/29224841-f856f665be51466d86680c75d016cbeb.png\" width=\"353\" height=\"56\"><\/p>\n\n<p>Logistic回归优缺点：<\/p>\n\n<ul>\n<li>优点：实现简单<\/li>\n<li>优点：分类时计算量非常小，速度很快，存储资源低<\/li>\n<li>缺点：容易欠拟合，一般准确度不太高<\/li>\n<li>缺点：只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分<\/li>\n<\/ul>\n\n<h3>线性回归<\/h3>\n\n<p>线性回归才是真正用于回归的，而不像logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normalequation直接求得参数的解，结果为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/54/eb3b7b2b4e35ce9b.png\" width=\"202\" height=\"59\"><\/p>\n\n<p>而在LWLR（局部加权线性回归）中，参数的计算表达式为:<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/55/270097b6b4631ab8.png\" width=\"260\" height=\"56\"><\/p>\n\n<p>因为此时优化的是：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/56/d78d05a1d560d3fb.png\" width=\"474\" height=\"95\"><\/p>\n\n<p>由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。<\/p>\n\n<p>线性回归优缺点：<\/p>\n\n<ul>\n<li>优点：实现简单，计算简单<\/li>\n<li>缺点：不能拟合非线性数据<\/li>\n<\/ul>\n\n<h3>KNN算法<\/h3>\n\n<p>KNN即最近邻算法，其主要过程为：<\/p>\n\n<ol>\n<li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；<\/li>\n<li>对上面所有的距离值进行排序；<\/li>\n<li>选前k个最小距离的样本；<\/li>\n<li>根据这k个样本的标签进行投票，得到最后的分类类别；<\/li>\n<\/ol>\n\n<p>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。<\/p>\n\n<p>近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。<\/p>\n\n<p>注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下：<\/p>\n\n<p><div class=\"lightbox-wrapper\"><a data-download-href=\"//cdn.suanfazu.com/uploads/default/a7b2249dc5cd0133f2437efa2059e9f336225824\" href=\"//cdn.suanfazu.com/uploads/default/57/f648583a7428f16d.png\" class=\"lightbox\" title=\"29225128-2a8eb5145f3c46f6aaba72544da7d9aa.png\"><img src=\"//cdn.suanfazu.com/uploads/default/_optimized/7b3/916/e634ca206f_690x190.png\" width=\"690\" height=\"190\"><div class=\"meta\">\n<span class=\"filename\">29225128-2a8eb5145f3c46f6aaba72544da7d9aa.png<\/span><span class=\"informations\">1140x315 26.8 KB<\/span><span class=\"expand\"><\/span>\n<\/div><\/a><\/div><\/p>\n\n<p>KNN算法的优缺点<\/p>\n\n<ul>\n<li>优点：思想简单，理论成熟，既可以用来做分类也可以用来做回归<\/li>\n<li>优点：可用于非线性分类<\/li>\n<li>优点：训练时间复杂度为O(n)<\/li>\n<li>优点：准确度高，对数据没有假设，对outlier不敏感<\/li>\n<li>缺点：计算量大<\/li>\n<li>缺点：样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）<\/li>\n<li>缺点：需要大量的内存<\/li>\n<\/ul>\n\n<h3>SVM<\/h3>\n\n<p>要学会如何使用libsvm以及一些参数的调节经验，另外需要理清楚svm算法的一些思路：<\/p>\n\n<ol>\n<li><p>svm中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？网易深度学习岗位面试过程中有被问到。答案就是几何间隔与样本的误分次数间存在关系：<img src=\"//cdn.suanfazu.com/uploads/default/58/e21f3d4ae0ef0596.png\" width=\"192\" height=\"73\">，其中的分母就是样本到分类间隔距离，分子中的R是所有样本中的最长向量值），即：<img src=\"//cdn.suanfazu.com/uploads/default/59/ebb053559cd79efa.png\" width=\"389\" height=\"91\"><br>经过一系列推导可得为优化下面原始目标：<img src=\"//cdn.suanfazu.com/uploads/default/60/2c062d10e30c66fd.png\" width=\"381\" height=\"87\"><\/p><\/li>\n<li><p>下面来看看拉格朗日理论：<img src=\"//cdn.suanfazu.com/uploads/default/61/b3fda0a6e57821e2.png\" width=\"556\" height=\"205\"><br>可以将1中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD条件），最后目标函数为：<img src=\"//cdn.suanfazu.com/uploads/default/62/be77f452468ead36.png\" width=\"443\" height=\"70\"><br>我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。<\/p><\/li>\n<li><p>对2中最后的式子分别w和b求导可得：<br><img src=\"//cdn.suanfazu.com/uploads/default/63/c67a6f285864d9fd.png\" width=\"159\" height=\"61\"><br><img src=\"//cdn.suanfazu.com/uploads/default/64/c035572477d2ecb8.png\" width=\"262\" height=\"69\"><span><\/span><br>由上面第1式子可以知道，如果我们优化出了α，则直接可以求出w了，即模型的参数搞定。而上面第2个式子可以作为后续优化的一个约束条件。<\/p><\/li>\n<li><p>对2中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数：<br><img src=\"//cdn.suanfazu.com/uploads/default/65/9eadd88470f4b46f.png\" width=\"461\" height=\"159\"><br>而这个函数可以用常用的优化方法求得α，进而求得w和b。<\/p><\/li>\n<\/ol>\n\n<p>5.按照道理，svm简单理论应该到此结束。不过还是要补充一点，即在预测时有：<br>  <img src=\"//cdn.suanfazu.com/uploads/default/66/825b2e8fde6d1192.png\" width=\"327\" height=\"136\"><br>  那个尖括号我们可以用核函数代替，这也是svm经常和核函数扯在一起的原因。<\/p>\n\n<ol><li>最后是关于松弛变量的引入，因此原始的目标优化公式为：<br><img src=\"//cdn.suanfazu.com/uploads/default/67/2ce64039a41629e5.png\" width=\"426\" height=\"125\"><br>此时对应的对偶优化公式为：<br><img src=\"//cdn.suanfazu.com/uploads/default/68/9e6fac528490476d.png\" width=\"454\" height=\"157\"><br>与前面的相比只是α多了个上界。<\/li><\/ol>\n\n<p>SVM算法优缺点<\/p>\n\n<ul>\n<li>优点：可用于线性/非线性分类，也可以用于回归<\/li>\n<li>优点：低泛化误差<\/li>\n<li>优点：容易解释<\/li>\n<li>优点：计算复杂度较低<\/li>\n<li>缺点：对参数和核函数的选择比较敏感<\/li>\n<li>缺点：原始的SVM只比较擅长处理二分类问题<\/li>\n<\/ul>\n\n<h3>Boosting<\/h3>\n\n<p>主要以Adaboost为例，首先来看看Adaboost的流程图，如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/69/9b036b1d241cac79.png\" width=\"\" height=\"\"><\/p>\n\n<p>从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为3个），每个弱分类器是由不同权重的样本（图中为5个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？<\/p>\n\n<p>下面通过一个例子来简单说明。<\/p>\n\n<p>书中（machine learning inaction）假设的是5个训练样本，每个训练样本的维度为2，在训练第一个分类器时5个样本的权重各为0.2.注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。<\/p>\n\n<p>现在假设弱分类器是带一个节点的简单决策树，该决策树会选择2个属性（假设只有2个属性）的一个，然后计算出这个属性中的最佳值用来分类。<\/p>\n\n<p>Adaboost的简单版本训练过程如下：<\/p>\n\n<p>1.训练第一个分类器，样本的权值D为相同的均值。通过一个弱分类器，得到这5个样本（请对应书中的例子来看，依旧是machinelearning inaction）的分类预测标签。与给出的样本真实标签对比，就可能出现误差(即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为0.最后累加5个样本的错误率之和，记为ε。<\/p>\n\n<ol><li>通过ε来计算该弱分类器的权重α，公式如下：<\/li><\/ol>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/70/46d21b97906fdf23.png\" width=\"188\" height=\"81\"><\/p>\n\n<p>3.通过α来计算训练下一个弱分类器样本的权重D，如果对应样本分类正确，则减小该样本的权重，公式为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/71/df8065d2cac1626b.png\" width=\"\" height=\"\"><\/p>\n\n<p>如果样本分类错误，则增加该样本的权重，公式为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/72/f1693cfba11058c1.png\" width=\"\" height=\"\"><\/p>\n\n<ol><li>循环步骤1,2,3来继续训练多个分类器，只是其D值不同而已。<\/li><\/ol>\n\n<p>测试过程如下：<\/p>\n\n<p>输入一个样本到训练好的每个弱分类中，则每个弱分类都对应一个输出标签，然后该标签乘以对应的α，最后求和得到值的符号即为预测标签值。<\/p>\n\n<p>Boosting算法的优缺点：<\/p>\n\n<ul>\n<li>优点：低泛化误差<\/li>\n<li>优点：容易实现，分类准确率较高，没有太多参数可以调<\/li>\n<li>缺点：对outlier比较敏感<\/li>\n<\/ul>\n\n<h3>聚类：<\/h3>\n\n<p>根据聚类思想划分：<\/p>\n\n<ol>\n<li>基于划分的聚类:<br>K-means,k-medoids(每一个类别中找一个样本点来代表),CLARANS.<br>k-means是使下面的表达式值最小：<br><img src=\"//cdn.suanfazu.com/uploads/default/73/d91ff1331bdb0dec.png\" width=\"\" height=\"\"><br>k-means算法的优缺点：<ul>\n<li><p>优点：k-means算法是解决聚类问题的一种经典算法，算法简单、快速<\/p><\/li>\n<li>优点：对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛<\/li>\n<li>优点：算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好<\/li>\n<li>缺点：k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合<\/li>\n<li>缺点：要求用户必须事先给出要生成的簇的数目k<\/li>\n<li>缺点：对初值敏感，对于不同的初始值，可能会导致不同的聚类结果<\/li>\n<li>缺点：不适合于发现非凸面形状的簇，或者大小差别很大的簇<\/li>\n<li>缺点：对于\"噪声\"和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响<\/li>\n<\/ul>\n<\/li>\n<li><p>基于层次的聚类：<br>自底向上的凝聚方法，比如AGNES。<br>自上向下的分裂方法，比如DIANA。<\/p><\/li>\n<li><p>基于密度的聚类：<br>DBSACN,OPTICS,BIRCH(CF-Tree),CURE.<\/p><\/li>\n<li><p>基于网格的方法：<br>STING, WaveCluster.<\/p><\/li>\n<li><p>基于模型的聚类：<br>EM,SOM,COBWEB.<\/p><\/li>\n<\/ol>\n\n<h3>推荐系统：<\/h3>\n\n<p>推荐系统的实现主要分为两个方面：基于内容的实现和协同滤波的实现。<\/p>\n\n<p><strong>基于内容的实现<\/strong><\/p>\n\n<p>不同人对不同电影的评分这个例子，可以看做是一个普通的回归问题，因此每部电影都需要提前提取出一个特征向量(即x值)，然后针对每个用户建模，即每个用户打的分值作为y值，利用这些已有的分值y和电影特征值x就可以训练回归模型了(最常见的就是线性回归)。这样就可以预测那些用户没有评分的电影的分数。（值得注意的是需对每个用户都建立他自己的回归模型）<\/p>\n\n<p>从另一个角度来看，也可以是先给定每个用户对某种电影的喜好程度（即权值），然后学出每部电影的特征，最后采用回归来预测那些没有被评分的电影。<\/p>\n\n<p>当然还可以是同时优化得到每个用户对不同类型电影的热爱程度以及每部电影的特征。具体可以参考Ng在coursera上的ml教程：https://www.coursera.org/course/ml<\/p>\n\n<p><strong>基于协同滤波的实现<\/strong><\/p>\n\n<p>协同滤波（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。<\/p>\n\n<p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/74/2ad28a75168d4b7b.png\" width=\"\" height=\"\"><\/p>\n\n<p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/75/ff909fdccebadfe5.png\" width=\"\" height=\"\"><\/p>\n\n<p>其中更深的颜色代表去掉小特征值重构时的三个矩阵。<\/p>\n\n<p>果m代表商品的个数，n代表用户的个数，则U矩阵的每一行代表商品的属性，现在通过降维U矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为k维）。这样当新来一个用户的商品推荐向量X，则可以根据公式X'<em>U1<\/em>inv(S1)得到一个k维的向量，然后在V’中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。具体例子可以参考网页：SVD在推荐系统中的应用。<\/p>\n\n<p>另外关于SVD分解后每个矩阵的实际含义可以参考google吴军的《数学之美》一书（不过个人感觉吴军解释UV两个矩阵时好像弄反了，不知道大家怎样认为）。或者参考machinelearning in action其中的svd章节。<\/p>\n\n<h3>pLSA:<\/h3>\n\n<p>pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。pLSA的模型图如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/76/7df43e6a15fcf8ce.png\" width=\"220\" height=\"74\"><\/p>\n\n<p>公式中的意义如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/77/818606e36fcb74b1.png\" width=\"551\" height=\"204\"><\/p>\n\n<h3>LDA<\/h3>\n\n<p>主题模型，概率图如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/78/17f37911653e1fc4.jpg\" width=\"438\" height=\"355\"><\/p>\n\n<p>和pLSA不同的是LDA中假设了很多先验分布，且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。<\/p>\n\n<h3>GDBT<\/h3>\n\n<p>GBDT(Gradient Boosting Decision Tree)又叫MART（Multiple Additive Regression Tree)，好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。<\/p>\n\n<p>GBDT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。<\/p>\n\n<p>关于GDBT的介绍可以参考 <br><aside class=\"quote\" data-post=\"1\" data-topic=\"133\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img width=\"20\" height=\"20\" src=\"/letter_avatar/king/40/2.png\" class=\"avatar\">\n    <a href=\"http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/133\">Gbdt 迭代决策树入门教程<\/a>  <a href=\"/category/ji-qi-xue-xi\" class=\"badge badge-category\" style=\"background-color: #3AB54A; color: #FFFFFF\">机器学习<\/a>\n  <\/div>\n  <blockquote>GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种用于回归的机器学习算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。当把目标函数做变换后，该算法亦可用于分类或排序。 \n\n本文主要从高层明确几个GBDT概念，主要讲GBDT的两个版本以及GBDT是什么不是什么。详细介绍见文中的链接。 \n\nGBDT的两个不同版本\n\n目前GBDT有两个不同的描述版本，两者各有支持者，读文献时要注意区分。残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差，之前我写的GBDT入门教程主要在描述这一版本，ELF开源软件实现中用的也是这一版本。Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值，之前leftnoteasy的博客中介绍的为此版本，umass的源码实现中用的则是这一版本（准确的说是LambdaMART中的MART为这一版本，MART实现则是前一版本）。 \n\n对GBDT无基础的朋友可…\n    <div class=\"topic-info\">\n    <\/div>\n  <\/blockquote>\n<\/aside>\n<\/p>\n\n<h3>Regularization:<\/h3>\n\n<p>作用是（网易电话面试时有问到）：<\/p>\n\n<ol>\n<li>数值上更容易求解<\/li>\n<li>特征数目太大时更稳定<br>3.控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑<\/li>\n<li>减小参数空间；参数空间越小，复杂度越低<br>5.系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）<\/li>\n<li>可以看出是权值的高斯先验<\/li>\n<\/ol>\n\n<h3>异常检测<\/h3>\n\n<p>可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。如果样本有n维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说x=log(x+c),x=x^(1/c)等。异常检测的算法流程如下：<\/p>\n\n<p><div class=\"lightbox-wrapper\"><a data-download-href=\"//cdn.suanfazu.com/uploads/default/e1f6bb7c8d1cff6d95504683556681719058f476\" href=\"//cdn.suanfazu.com/uploads/default/79/7e2de99b523dbff9.png\" class=\"lightbox\" title=\"29230143-aa4642e8d66a468b9138e9014b2ec704.png\"><img src=\"//cdn.suanfazu.com/uploads/default/_optimized/71b/c66/2846715f92_690x415.png\" width=\"690\" height=\"415\"><div class=\"meta\">\n<span class=\"filename\">29230143-aa4642e8d66a468b9138e9014b2ec704.png<\/span><span class=\"informations\">813x490 66.5 KB<\/span><span class=\"expand\"><\/span>\n<\/div><\/a><\/div><\/p>\n\n<p>其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的p(x)的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。<\/p>\n\n<p>另外，上面是将特征的每一维看成是相互独立的高斯分布，其实这样的近似并不是最好的，但是它的计算量较小，因此也常被使用。更好的方法应该是将特征拟合成多维高斯分布，这时有特征之间的相关性，但随之计算量会变复杂，且样本的协方差矩阵还可能出现不可逆的情况（主要在样本数比特征数小，或者样本特征维数之间有线性关系时）。<\/p>\n\n<p>上面的内容可以参考Ng的 <a href=\"https://www.coursera.org/course/ml\" rel=\"nofollow\">https://www.coursera.org/course/ml<\/a><\/p>\n\n<h3>EM算法<\/h3>\n\n<p>有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：<\/p>\n\n<ol>\n<li>E步：选取一组参数，求出在该参数下隐含变量的条件概率值<\/li>\n<li>M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值<\/li>\n<\/ol>\n\n<p>重复上面2步直至收敛<\/p>\n\n<p>公式如下所示：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/80/9cf57f443d28dd5a.png\" width=\"\" height=\"\"><\/p>\n\n<p>M步公式中下界函数的推导过程：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/81/e1d8f6d9384a55b1.png\" width=\"\" height=\"\"><\/p>\n\n<p>EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。<\/p>\n\n<p>GMM的E步公式如下（计算每个样本对应每个高斯的概率）：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/82/cebc0e1f7f830dbc.png\" width=\"\" height=\"\"><\/p>\n\n<p>更具体的计算公式为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/83/359c905e05fde36f.png\" width=\"\" height=\"\"><\/p>\n\n<p>M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/84/bb2dd6bdcbb1bc0b.png\" width=\"\" height=\"\"><\/p>\n\n<p>关于EM算法可以参考Ng的cs229课程资料 或者网易公开课：斯坦福大学公开课：机器学习课程。<\/p>\n\n<h3>Apriori:<\/h3>\n\n<p>Apriori是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：<\/p>\n\n<p>1.如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合<br>2.如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合<\/p>\n\n<p>Aprioir需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为L，然后对L中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为C，接着又扫描去掉那些非频繁的项目，重复…<\/p>\n\n<p>看下面这个例子：<\/p>\n\n<p>元素项目表格：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/85/940f014b9738148a.png\" width=\"\" height=\"\"><\/p>\n\n<p>如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/86/01a37040a7ca3a44.png\" width=\"\" height=\"\"><\/p>\n\n<p>在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/87/86a71a3e8aef68d1.png\" width=\"360\" height=\"342\"><\/p>\n\n<p>上面的内容主要参考的是machine learningin action这本书。<\/p>\n\n<h3>FPGrowth:<\/h3>\n\n<p>FPGrowth是一种比Apriori更高效的频繁项挖掘方法，它只需要扫描项目表2次。其中第1次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第2遍扫描是建立一颗FP-Tree(frequent-pattentree)。<\/p>\n\n<p>接下来的工作就是在FP-Tree上进行挖掘。<\/p>\n\n<p>比如说有下表：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/88/4c045a7c07f58cde.png\" width=\"537\" height=\"239\"><\/p>\n\n<p>它所对应的FP_Tree如下：<\/p>\n\n<p><img src=\"//cdn.suanfazu.com/uploads/default/89/2a125f846556f88a.png\" width=\"\" height=\"\"><\/p>\n\n<p>然后从频率最小的单项P开始，找出P的条件模式基，用构造FP_Tree同样的方法来构造P的条件模式基的FP_Tree，在这棵树上找出包含P的频繁项集。<\/p>\n\n<p>依次从m,b,a,c,f的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如m节点，具体的过程可以参考博客：FrequentPattern挖掘之二(FPGrowth算法)，里面讲得很详细。<\/p>","post_number":1,"post_type":1,"updated_at":"2014-12-22T05:27:16.760Z","reply_count":0,"reply_to_post_number":null,"quote_count":0,"avg_time":63,"incoming_link_count":203,"reads":122,"score":1257.55,"yours":false,"topic_id":134,"topic_slug":"mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li","display_username":"king","primary_group_name":null,"version":2,"can_edit":false,"can_delete":false,"can_recover":false,"can_wiki":false,"link_counts":[{"url":"http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/133","internal":true,"reflection":false,"title":"Gbdt（mart）概念简介","clicks":23},{"url":"https://www.coursera.org/course/ml","internal":false,"reflection":false,"title":"Coursera","clicks":1}],"read":true,"user_title":null,"actions_summary":[{"id":2,"count":6}],"moderator":false,"admin":false,"staff":false,"user_id":54,"hidden":false,"hidden_reason_id":null,"trust_level":1,"deleted_at":null,"user_deleted":false,"edit_reason":"下载外部图片留作副本","can_view_edit_history":false,"wiki":false}],"stream":[138]},"id":134,"title":"面试之机器学习算法思想简单梳理","fancy_title":"面试之机器学习算法思想简单梳理","posts_count":1,"created_at":"2014-12-22T05:17:53.221Z","views":2643,"reply_count":0,"participant_count":1,"like_count":6,"last_posted_at":"2014-12-22T05:17:53.583Z","visible":true,"closed":false,"archived":false,"has_summary":false,"archetype":"regular","slug":"mian-shi-zhi-ji-qi-xue-xi-suan-fa-si-xiang-jian-dan-shu-li","category_id":12,"word_count":705,"deleted_at":null,"user_id":54,"draft":null,"draft_key":"topic_134","draft_sequence":null,"unpinned":null,"pinned_globally":false,"pinned":false,"pinned_at":null,"pinned_until":null,"details":{"auto_close_at":null,"auto_close_hours":null,"auto_close_based_on_last_post":false,"created_by":{"id":54,"username":"king","avatar_template":"/letter_avatar_proxy/v2/letter/k/a9adbd/{size}.png"},"last_poster":{"id":54,"username":"king","avatar_template":"/letter_avatar_proxy/v2/letter/k/a9adbd/{size}.png"},"participants":[{"id":54,"username":"king","avatar_template":"/letter_avatar_proxy/v2/letter/k/a9adbd/{size}.png","post_count":1}],"suggested_topics":[{"id":99,"title":"用两个栈实现一个队列-百度","fancy_title":"用两个栈实现一个队列-百度","slug":"yong-liang-ge-zhan-shi-xian-ge-dui-lie-bai-du","posts_count":2,"reply_count":0,"highest_post_number":2,"image_url":null,"created_at":"2014-12-18T03:04:34.377Z","last_posted_at":"2014-12-21T16:48:54.849Z","bumped":true,"bumped_at":"2014-12-21T16:48:54.849Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":1,"views":1118,"category_id":12},{"id":101,"title":"二叉树两个叶子节点间的路径-百度","fancy_title":"二叉树两个叶子节点间的路径-百度","slug":"topic","posts_count":10,"reply_count":6,"highest_post_number":11,"image_url":null,"created_at":"2014-12-18T03:08:43.748Z","last_posted_at":"2016-05-03T09:47:45.683Z","bumped":true,"bumped_at":"2016-05-03T09:47:45.683Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":1,"views":4904,"category_id":12},{"id":103,"title":"求字符串中第一个只出现一次的字符-百度","fancy_title":"求字符串中第一个只出现一次的字符-百度","slug":"qiu-zi-fu-chuan-zhong-di-ge-zhi-chu-xian-ci-de-zi-fu-bai-du","posts_count":1,"reply_count":0,"highest_post_number":1,"image_url":null,"created_at":"2014-12-18T03:09:50.069Z","last_posted_at":"2014-12-18T03:09:50.178Z","bumped":true,"bumped_at":"2014-12-18T03:09:50.178Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":0,"views":1337,"category_id":12},{"id":100,"title":"求二叉树的宽度-百度","fancy_title":"求二叉树的宽度-百度","slug":"qiu-er-cha-shu-de-kuan-du-bai-du","posts_count":1,"reply_count":0,"highest_post_number":1,"image_url":null,"created_at":"2014-12-18T03:06:41.061Z","last_posted_at":"2014-12-18T03:06:41.294Z","bumped":true,"bumped_at":"2014-12-18T03:06:41.294Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":0,"views":1025,"category_id":12},{"id":108,"title":"求先递增后递减数组的最大值-美团","fancy_title":"求先递增后递减数组的最大值-美团","slug":"qiu-xian-di-zeng-hou-di-jian-shu-zu-de-zui-da-zhi-mei-tuan","posts_count":2,"reply_count":0,"highest_post_number":2,"image_url":null,"created_at":"2014-12-18T03:21:34.814Z","last_posted_at":"2014-12-21T16:40:44.808Z","bumped":true,"bumped_at":"2014-12-21T16:40:44.808Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":1,"views":1629,"category_id":12},{"id":124,"title":"重叠矩阵求面积","fancy_title":"重叠矩阵求面积","slug":"topic","posts_count":2,"reply_count":0,"highest_post_number":2,"image_url":null,"created_at":"2014-12-18T15:26:21.616Z","last_posted_at":"2015-07-21T13:19:00.461Z","bumped":true,"bumped_at":"2015-07-21T13:19:00.461Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":0,"views":1867,"category_id":12},{"id":207,"title":"码农翻墙去美帝――数学和机器学习准备","fancy_title":"码农翻墙去美帝――数学和机器学习准备","slug":"ma-nong-fan-qiang-qu-mei-di-shu-xue-he-ji-qi-xue-xi-zhun-bei","posts_count":1,"reply_count":0,"highest_post_number":1,"image_url":null,"created_at":"2014-12-26T17:46:33.010Z","last_posted_at":"2014-12-26T17:46:33.078Z","bumped":true,"bumped_at":"2014-12-26T17:46:33.078Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":2,"views":2886,"category_id":12},{"id":297,"title":"互联网公司机器学习职位面试考察点","fancy_title":"互联网公司机器学习职位面试考察点","slug":"hu-lian-wang-gong-si-ji-qi-xue-xi-zhi-wei-mian-shi-kao-cha-dian","posts_count":1,"reply_count":0,"highest_post_number":1,"image_url":"/uploads/default/205/3a9ba0671a47c382.png","created_at":"2015-01-05T05:23:21.639Z","last_posted_at":"2015-01-05T05:23:21.783Z","bumped":true,"bumped_at":"2015-01-05T05:23:21.783Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":1,"views":1463,"category_id":12},{"id":14129,"title":"分享一套机器学习的题目和非常详细的解析过程","fancy_title":"分享一套机器学习的题目和非常详细的解析过程","slug":"topic","posts_count":5,"reply_count":3,"highest_post_number":5,"image_url":null,"created_at":"2016-09-08T07:02:53.752Z","last_posted_at":"2016-09-20T06:56:32.630Z","bumped":true,"bumped_at":"2016-09-20T06:56:32.630Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":3,"views":2115,"category_id":12},{"id":107,"title":"ip转城市-微软","fancy_title":"ip转城市-微软","slug":"ipzhuan-cheng-shi-wei-ruan","posts_count":2,"reply_count":0,"highest_post_number":2,"image_url":null,"created_at":"2014-12-18T03:18:48.599Z","last_posted_at":"2014-12-21T16:37:56.025Z","bumped":true,"bumped_at":"2014-12-21T16:37:56.025Z","unseen":false,"pinned":false,"unpinned":null,"visible":true,"closed":false,"archived":false,"bookmarked":null,"liked":null,"archetype":"regular","like_count":0,"views":1380,"category_id":12}],"links":[{"url":"http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/133","title":"Gbdt（mart）概念简介","fancy_title":null,"internal":true,"attachment":false,"reflection":false,"clicks":23,"user_id":54,"domain":"suanfazu.com"},{"url":"https://www.coursera.org/course/ml","title":"Coursera","fancy_title":null,"internal":false,"attachment":false,"reflection":false,"clicks":1,"user_id":54,"domain":"www.coursera.org"}],"notification_level":1,"can_flag_topic":false},"highest_post_number":1,"deleted_by":null,"actions_summary":[{"id":4,"count":0,"hidden":false,"can_act":false},{"id":7,"count":0,"hidden":false,"can_act":false},{"id":8,"count":0,"hidden":false,"can_act":false}],"chunk_size":20,"bookmarked":null,"tags":null});
      </script> 
  <script>
  window.assetPath = (function(){
    var map = {"defer/html-sanitizer-bundle":"//cdn.suanfazu.com/assets/defer/html-sanitizer-bundle-d248c5e7fffd65438fab42fafa3d1d56.js"};
    return function(asset) { return map[asset]; };
  })();
</script> 
  <script>
  Ember.RSVP.configure('onerror', function(e) {
    // Ignore TransitionAborted exceptions that bubble up
    if (e && e.message === "TransitionAborted") { return; }

    window.onerror(e && e.message, null,null,null,e);
  });


</script> 
  <script>
  Discourse.CDN = '//cdn.suanfazu.com';
  Discourse.BaseUrl = 'suanfazu.com'.replace(/:[\d]*$/,"");
  Discourse.BaseUri = '';
  Discourse.Environment = 'production';
  Discourse.SiteSettings = PreloadStore.get('siteSettings');
  Discourse.LetterAvatarVersion = '5_b81b9db9c25c85a3cb45c5969b40553e';
  I18n.defaultLocale = 'zh_CN';
  PreloadStore.get("customEmoji").forEach(function(emoji) {
    Discourse.Dialect.registerEmoji(emoji.name, emoji.url);
  });
  Discourse.start();
  Discourse.set('assetVersion','a26913625110801f581a18c324969d26');
  Discourse.Session.currentProp("disableCustomCSS", false);
  Discourse.HighlightJSPath = "/highlight-js/suanfazu.com/133b1767dbeecf92cad6dfc38d42cde22195db05.js";
</script> 
  <script src="//cdn.suanfazu.com/assets/browser-update-1b088c371e098d02d2b87570660d5d68.js"></script> 
  <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?9b0738ab1116d7971e6048c2c63c1da4";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script> 
  <script>
function sfz_wait(required_selector, excluded_selector, check_interval, callback) {
    var func = function(){sfz_wait(required_selector, excluded_selector, check_interval, callback)};

    var s=$(required_selector);
    if (!s || s.length <= 0) {
        callback(false);
        setTimeout(func, check_interval);
        return;
    }

    s=$(excluded_selector);
    if (s && s.length > 0) {
        setTimeout(func, check_interval * 2);
        return;
    }
    
    callback(true);
    setTimeout(func, check_interval * 2);
}

function _parse_toc(dom) {
    var contents = $('#main-outlet div.post > ul:first', $('<html>').html($('noscript', dom).text()));
    if (contents && contents.length > 0) return contents;
    var s = $('script:contains("PreloadStore.store"):first', dom).text();
    s = JSON.parse(s.substring(s.indexOf(',') + 1, s.lastIndexOf(')'))).post_stream.posts[0].cooked;
    contents = $('ul:first', $('<html>').html(s));
    if (contents && contents.length > 0) return contents;
    return null;
}

var TOCTAG='::::';
function handle_toc(enabled) {
    $('#toc-left-container').remove();
    if (!enabled) return;
    var toc = $('#post_1 a:contains("' + TOCTAG + '")');
    var toc_url = toc.attr('href');
    if (!toc_url || (toc_url.indexOf('://') != -1 && toc_url.indexOf('http://suanfazu.com/') != 0)) {return;};
    var json_url = null;
    try {
        json_url = toc_url.split('/t/')[1].split('/')[1];
        json_url = 'http://suanfazu.com/t/' + json_url + '.json'
    } catch (e) {console.log(e);};
    $.getJSON({
        url: json_url,
        cache: true,
        context: document.body,
        success: function(data, textStatus, request) {
            try {
                var contents = $('body > ul:first', $('<html>').html(data.post_stream.posts[0].cooked));
                if (!contents) {return;}
                contents.attr('class', 'toc toc-body');
                toc.after(contents);

                if (screen.width >= 1360) {
                    var contents = $('body > ul:first', $('<html>').html(data.post_stream.posts[0].cooked));
                    contents.attr('class', 'toc toc-left');
                    var title = toc.clone().children().remove().end().text().replace(TOCTAG, '').trim();
                    title = title.replace('<', '&lt;').replace('>', '&gt;');
                    $('body').append('<div id="toc-left-container"><div class="toc-title"><a href="' + toc_url + '">' + title + '</a></div></div>');
                    $('#toc-left-container').append(contents);
                    if ($('#toc-left-container').height() >= screen.height - 100) {
                        $('#toc-left-container').css({top:70});
                    } else {
                        $(window).scroll(function () {
                            var currenttop = $(window).scrollTop();
                            if(currenttop>180){
                                $('#toc-left-container').css({position:'fixed', top:29, zIndex:100000});
                            }
                            if($('#toc-left-container').css('position')=='fixed'){
                                if(currenttop<180) {
                                    $('#toc-left-container').css({position:'absolute', top:200});
                                }
                            }
                        });
                    }
                }

                var curr = $('.toc a[href*="' + window.location.href.replace('http://suanfazu.com', '') + '"]');
                curr.attr('class', 'toc-curr-a');
                curr.parent().attr('class', 'toc-curr');
                toc.after('<div class="toc-page"><div id="toc-prev"></div><div id="toc-next"></div></div>');
                var items = $('.toc-body a');
                for (var i = 0; i < items.length; i++) {
                    if ($(items[i]).attr('class') == 'toc-curr-a') {
                        if (i > 0) {
                            $('#toc-prev').append('<span>上一篇：</span><a href="' + $(items[i-1]).attr('href') + '">' + $(items[i-1]).text() + '</a>');
                        }
                        if (i + 1 < items.length) {
                            $('#toc-next').append('<span>下一篇：</span><a href="' + $(items[i+1]).attr('href') + '">' + $(items[i+1]).text() + '</a>');
                        }
                        break;
                    }
                }
                // $.merge($('.toc-page a'), $('.toc a')).click(function(){
                //     window.location.href = $(this).attr('href');
                // });
            } catch (e) {console.log(e);};
        }
    });
}
//handle_toc();
//if (window.location.href.indexOf('suanfazu.com/t/') != -1) 
{sfz_wait('#post_1', '.toc-body', 500, handle_toc);}
</script>   
 </body>
</html>